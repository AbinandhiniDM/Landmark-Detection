# -*- coding: utf-8 -*-
"""Landmark_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H3vh6gQ64qGNEX4BnkUjG0QiE-3oZT-y
"""

!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local

import sys
sys.path.append('/usr/local/lib/python3.8/site-packages/')

!conda init

!conda create -n PyTorch python=3.8
!conda activate PyTorch
!conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch-lts

import torch
print(torch.cuda.is_available())

!pip install timm

!pip install opencv-python
!pip install PyYAML
!pip install tqdm

!unzip afw.zip -d /content/PIPNet/dataset/afw
!unzip ibug.zip -d /content/PIPNet/dataset/ibug
!unzip lfpw.zip -d /content/PIPNet/dataset/lfpw
!unzip helen.zip -d /content/PIPNet/dataset/helen

from sys import path
path.append('/content/PIPNet/utils')
from util import DataGenerator

# Use the DataGenerator class
gen = DataGenerator("/content/PIPNet/dataset/")
gen.run()

class Dataset(dataset):
    def __init__(self,
                 params,
                 data_dir,
                 augment=True):

        self.params = params
        self.augment = augment
        self.data_dir = data_dir
        self.samples = self.load_label(self.data_dir)
        self.mean_indices = compute_indices(join(dirname(data_dir), 'indices.txt'), params)[0]

        self.resize = T.Resize((self.params['input_size'], self.params['input_size']))
        self.normalize = T.Compose(
            [self.resize, T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])

        self.transforms = (RandomHSV(),
                           RandomRotate(),
                           RandomCutOut(),
                           RandomRGB2IR(),
                           RandomTranslate(),
                           RandomFlip(params),
                           RandomGaussianBlur())

    def __getitem__(self, item):
        img_name, label = self.samples[item]
        image = Image.open(os.path.join(img_name)).convert('RGB')

        if not self.augment:
            image = self.normalize(image)
            return image, label

        for transform in self.transforms:
            image, label = transform(image, label)

        image = self.normalize(image)

        reduced_size = int(self.params['input_size'] / self.params['stride'])

        target_map = np.zeros((self.params['num_lms'], reduced_size, reduced_size))
        target_local_x = np.zeros((self.params['num_lms'], reduced_size, reduced_size))
        target_local_y = np.zeros((self.params['num_lms'], reduced_size, reduced_size))
        target_nb_x = np.zeros((self.params['num_nb'] * self.params['num_lms'], reduced_size, reduced_size))
        target_nb_y = np.zeros((self.params['num_nb'] * self.params['num_lms'], reduced_size, reduced_size))
        ground_points = target_map, target_local_x, target_local_y, target_nb_x, target_nb_y

        target_map, target_local_x, target_local_y, target_nb_x, target_nb_y = self.init_target(label,
                                                                                                self.mean_indices,
                                                                                                self.params['num_nb'],
                                                                                                ground_points)

        target_map = torch.from_numpy(target_map).float()
        target_local_x = torch.from_numpy(target_local_x).float()
        target_local_y = torch.from_numpy(target_local_y).float()
        target_nb_x = torch.from_numpy(target_nb_x).float()
        target_nb_y = torch.from_numpy(target_nb_y).float()

        return image, (target_map, target_local_x, target_local_y, target_nb_x, target_nb_y)

    def __len__(self):
        return len(self.samples)

    @staticmethod
    def load_label(data_dir):
        with open(data_dir, 'r') as f:
            labels = f.readlines()
        labels = [x.strip().split() for x in labels]
        if len(labels[0]) == 1:
            return labels

        labels_new = []
        for label in labels:
            image_name, target = label[0], label[1:]
            labels_new.append([image_name, np.array([float(x) for x in target])])
        return labels_new

data_dir = '/content/dataset/'
# Create the dataset object
dataset = Dataset(params=params, data_dir=data_dir, augment=False)
# Get a single item from the dataset
image, labels = dataset[0]  # Index 0 to get the first image
# Convert the tensor to a NumPy array for visualization
image = image.permute(1, 2, 0).numpy()  # Change dimensions from (C, H, W) to (H, W, C)
image = image * 255  # De-normalize if needed
image = image.astype('uint8')
# Plot the image
Image(image)

"""**AGENT AND ENVIRONMENT**"""

import os
import sys
import six
import random
import threading
import numpy as np
from tensorpack import logger
from collections import (Counter, defaultdict, deque, namedtuple)

import cv2
import math
import time
from PIL import Image
import subprocess
import shutil

import gym
from gym import spaces
from tensorpack.utils.utils import get_rng
from tensorpack.utils.stats import StatCounter

from IPython.core.debugger import set_trace
from dataReader import *

def __init__(self, directory=None, viz=False, task=False, files_list=None,
                 screen_dims=(27,27,27), history_length=20, multiscale=True,
                 max_num_frames=0, saveGif=False, saveVideo=False):
  # inits stat counters
  self.reset_stat()
  # counter to limit number of steps per episodes
  self.cnt = 0
  # maximum number of frames (steps) per episodes
  self.max_num_frames = max_num_frames
  # stores information: terminal, score, distError
  self.info = None
  # option to save display as gif
  self.saveGif = saveGif
  # training flag
  self.task = task
  # image dimension (2D/3D)
  self.screen_dims = screen_dims
  self.dims = len(self.screen_dims)
  # multi-scale agent
  self.multiscale = multiscale
  # init env dimensions
  if self.dims == 2:
      self.width, self.height = screen_dims
  else:
      self.width, self.height, self.depth = screen_dims
  # stat counter to store current score or accumlated reward
  self.current_episode_score = StatCounter()
  # get action space and minimal action set
  self.action_space = spaces.Discrete(6)  # change number actions here
  self.actions = self.action_space.n
  self.observation_space = spaces.Box(low=0, high=255,
                                      shape=self.screen_dims,
                                      dtype=np.uint8)
  # history buffer for storing last locations to check oscilations
  self._history_length = history_length
  self._loc_history = [(0,) * self.dims] * self._history_length
  self._qvalues_history = [(0,) * self.actions] * self._history_length
  # initialize rectangle limits from input image coordinates
  self.rectangle = Rectangle(0, 0, 0, 0, 0, 0)
  # add your data loader here
  if self.task == 'play':
      self.files = filesListBrainMRLandmark(files_list,
                                            returnLandmarks=False)
  else:
      self.files = filesListBrainMRLandmark(files_list,
                                            returnLandmarks=True)
  # prepare file sampler
  self.filepath = None
  self.sampled_files = self.files.sample_circular()
  # reset buffer, terminal, counters, and init new_random_game
  self._restart_episode()

def _restart_episode(self):
        """
        restart current episoide
        """
        self.terminal = False
        self.reward = 0
        self.cnt = 0 # counter to limit number of steps per episodes
        self.num_games.feed(1)
        self.current_episode_score.reset()  # reset the stat counter
        self._loc_history = [(0,) * self.dims] * self._history_length
        # list of q-value lists
        self._qvalues_history = [(0,) * self.actions] * self._history_length
        self.new_random_game()

def new_random_game(self):
  self._image, self._target_loc, self.filepath, self.spacing = next(self.sampled_files)
  self.filename = os.path.basename(self.filepath)
  # multiscale (e.g. start with 3 -> 2 -> 1)
  # scale can be thought of as sampling stride
  if self.multiscale:
    self.action_step = 9
    self.xscale = 3
    self.yscale = 3
    self.zscale = 3
  else:
    self.action_step = 1
    self.xscale = 1
    self.yscale = 1
    self.zscale = 1
  # image volume size
  self._image_dims = self._image.dims
  self.cur_dist = self.calcDistance(self._location,
                                              self._target_loc,
                                              self.spacing)

def step(self, act, qvalues):
  self._qvalues = qvalues
  current_loc = self._location
  self.terminal = False
  go_out = False
  # UP Z+
  if (act == 0):
      next_location = (current_loc[0],
                        current_loc[1],
                        round(current_loc[2] + self.action_step))
      if (next_location[2] >= self._image_dims[2]):
          # print(' trying to go out the image Z+ ',)
          next_location = current_loc
          go_out = True
  # FORWARD Y+
  if (act == 1):
      next_location = (current_loc[0],
                        round(current_loc[1] + self.action_step),
                        current_loc[2])
      if (next_location[1] >= self._image_dims[1]):
          # print(' trying to go out the image Y+ ',)
          next_location = current_loc
          go_out = True
  # RIGHT X+
  if (act == 2):
      next_location = (round(current_loc[0] + self.action_step),
                        current_loc[1],
                        current_loc[2])
      if next_location[0] >= self._image_dims[0]:
          # print(' trying to go out the image X+ ',)
          next_location = current_loc
          go_out = True
  # LEFT X-
  if act == 3:
      next_location = (round(current_loc[0] - self.action_step),
                        current_loc[1],
                        current_loc[2])
      if next_location[0] <= 0:
          # print(' trying to go out the image X- ',)
          next_location = current_loc
          go_out = True
  # BACKWARD Y-
  if act == 4:
      next_location = (current_loc[0],
                        round(current_loc[1] - self.action_step),
                        current_loc[2])
      if next_location[1] <= 0:
          # print(' trying to go out the image Y- ',)
          next_location = current_loc
          go_out = True
  # DOWN Z-
  if act == 5:
      next_location = (current_loc[0],
                        current_loc[1],
                        round(current_loc[2] - self.action_step))
      if next_location[2] <= 0:
          # print(' trying to go out the image Z- ',)
          next_location = current_loc
          go_out = True
  # punish -1 reward if the agent tries to go out
  if (self.task!='play'):
      if go_out:
          self.reward = -1
      else:
          self.reward = self._calc_reward(current_loc, next_location)
  # update screen, reward ,location, terminal
  self._location = next_location
  self._screen = self._current_state()
  # terminate if the distance is less than 1 during trainig
  if (self.task == 'train'):
      if self.cur_dist <= 1:
          self.terminal = True
          self.num_success.feed(1)
  # terminate if maximum number of steps is reached
  self.cnt += 1
  if self.cnt >= self.max_num_frames: self.terminal = True
  # update history buffer with new location and qvalues
  if (self.task != 'play'):
      self.cur_dist = self.calcDistance(self._location,
                                        self._target_loc,
                                        self.spacing)
  self._update_history()
  # check if agent oscillates
  if self._oscillate:
      self._location = self.getBestLocation()
      self._screen = self._current_state()

      if (self.task != 'play'):
          self.cur_dist = self.calcDistance(self._location,
                                            self._target_loc,
                                            self.spacing)
      # multi-scale steps
      if self.multiscale:
          if self.xscale > 1:
              self.xscale -= 1
              self.yscale -= 1
              self.zscale -= 1
              self.action_step = int(self.action_step / 3)
              self._clear_history()
          # terminate if scale is less than 1
          else:
              self.terminal = True
              if self.cur_dist <= 1: self.num_success.feed(1)
      else:
          self.terminal = True
          if self.cur_dist <= 1: self.num_success.feed(1)
  distance_error = self.cur_dist
  self.current_episode_score.feed(self.reward)

  info = {'score': self.current_episode_score.sum, 'gameOver': self.terminal,
          'distError': distance_error, 'filename': self.filename}

"""**STATE SPACE AND REWARD CALCULATION**"""

def _update_history(self):
        ''' update history buffer with current state
        '''
        # update location history
        self._loc_history[:-1] = self._loc_history[1:]
        self._loc_history[-1] = self._location
        # update q-value history
        self._qvalues_history[:-1] = self._qvalues_history[1:]
        self._qvalues_history[-1] = self._qvalues

def _current_state(self):
    # initialize screen with zeros - all background
    screen = np.zeros((self.screen_dims)).astype(self._image.data.dtype)

    # screen uses coordinate system relative to origin (0, 0, 0)
    screen_xmin, screen_ymin, screen_zmin = 0, 0, 0
    screen_xmax, screen_ymax, screen_zmax = self.screen_dims

    # extract boundary locations using coordinate system relative to "global" image
    # width, height, depth in terms of screen coord system
    if self.xscale % 2:
        xmin = self._location[0] - int(self.width * self.xscale / 2) - 1
        xmax = self._location[0] + int(self.width * self.xscale / 2)
        ymin = self._location[1] - int(self.height * self.yscale / 2) - 1
        ymax = self._location[1] + int(self.height * self.yscale / 2)
        zmin = self._location[2] - int(self.depth * self.zscale / 2) - 1
        zmax = self._location[2] + int(self.depth * self.zscale / 2)
    else:
        xmin = self._location[0] - round(self.width * self.xscale / 2)
        xmax = self._location[0] + round(self.width * self.xscale / 2)
        ymin = self._location[1] - round(self.height * self.yscale / 2)
        ymax = self._location[1] + round(self.height * self.yscale / 2)
        zmin = self._location[2] - round(self.depth * self.zscale / 2)
        zmax = self._location[2] + round(self.depth * self.zscale / 2)

    # check if they violate image boundary and fix it
    if xmin < 0:
        xmin = 0
        screen_xmin = screen_xmax - len(np.arange(xmin, xmax, self.xscale))
    if ymin < 0:
        ymin = 0
        screen_ymin = screen_ymax - len(np.arange(ymin, ymax, self.yscale))
    if zmin < 0:
        zmin = 0
        screen_zmin = screen_zmax - len(np.arange(zmin, zmax, self.zscale))
    if xmax > self._image_dims[0]:
        xmax = self._image_dims[0]
        screen_xmax = screen_xmin + len(np.arange(xmin,xmax,self.xscale))
    if ymax>self._image_dims[1]:
        ymax = self._image_dims[1]
        screen_ymax = screen_ymin + len(np.arange(ymin,ymax,self.yscale))
    if zmax>self._image_dims[2]:
        zmax = self._image_dims[2]
        screen_zmax = screen_zmin + len(np.arange(zmin,zmax,self.zscale))

    screen[screen_xmin:screen_xmax, screen_ymin:screen_ymax, screen_zmin:screen_zmax] = self._image.data[
                                                                                        xmin:xmax:self.xscale,
                                                                                        ymin:ymax:self.yscale,
                                                                                        zmin:zmax:self.zscale]
    self.rectangle = Rectangle(xmin, xmax,
                                ymin, ymax,
                                zmin, zmax)

    return screen

def _calc_reward(self, current_loc, next_loc):
        """ Calculate the new reward based on the decrease in euclidean distance to the target location
        """
        curr_dist = self.calcDistance(current_loc, self._target_loc,
                                      self.spacing)
        next_dist = self.calcDistance(next_loc, self._target_loc,
                                      self.spacing)
        return curr_dist - next_dist

@property
def _oscillate(self):
    """ Return True if the agent is stuck and oscillating
    """
    counter = Counter(self._loc_history)
    freq = counter.most_common()

    if freq[0][0] == (0, 0, 0):
        if (freq[1][1] > 3):
            return True
        else:
            return False
    elif (freq[0][1] > 3):
        return True

def get_action_meanings(self):
    """ return array of integers for actions"""
    ACTION_MEANING = {
        1: "UP",  # MOVE Z+
        2: "FORWARD",  # MOVE Y+
        3: "RIGHT",  # MOVE X+
        4: "LEFT",  # MOVE X-
        5: "BACKWARD",  # MOVE Y-
        6: "DOWN",  # MOVE Z-
    }
    return [ACTION_MEANING[i] for i in self.actions]

"""**DQN MODEL**"""

import tensorflow as tf
def __init__(self, image_shape, channel, method, num_actions, gamma):
    self.gamma = gamma
    self.method = method
    self.channel = channel
    self.image_shape = image_shape
    self.num_actions = num_actions

def inputs(self):
    # Use a combined state for efficiency.
    # The first h channels are the current state, and the last h channels are the next state.
    return [InputDesc(tf.uint8,
                      (None,) + self.image_shape + (self.channel + 1,),
                      'comb_state'),
            InputDesc(tf.int64, (None,), 'action'),
            InputDesc(tf.float32, (None,), 'reward'),
            InputDesc(tf.bool, (None,), 'isOver')]

def build_graph(self, *inputs):
    comb_state, action, reward, isOver = inputs
    comb_state = tf.cast(comb_state, tf.float32)
    state = tf.slice(comb_state, [0, 0, 0, 0, 0], [-1, -1, -1, -1, self.channel], name='state')
    self.predict_value = self.get_DQN_prediction(state)
    if not get_current_tower_context().is_training:
        return

    reward = tf.clip_by_value(reward, -1, 1)
    next_state = tf.slice(comb_state, [0, 0, 0, 0, 1], [-1, -1, -1, -1, self.channel], name='next_state')
    action_onehot = tf.one_hot(action, self.num_actions, 1.0, 0.0)

    pred_action_value = tf.reduce_sum(self.predict_value * action_onehot, 1)  # N,
    max_pred_reward = tf.reduce_mean(tf.reduce_max(
        self.predict_value, 1), name='predict_reward')
    summary.add_moving_summary(max_pred_reward)

    with tf.variable_scope('target'):
        targetQ_predict_value = self.get_DQN_prediction(next_state)  # NxA

    if 'Double' not in self.method:
        # DQN or Dueling
        best_v = tf.reduce_max(targetQ_predict_value, 1)  # N,
    else:
        # Double-DQN or DuelingDouble
        next_predict_value = self.get_DQN_prediction(next_state)
        self.greedy_choice = tf.argmax(next_predict_value, 1)  # N,
        predict_onehot = tf.one_hot(self.greedy_choice, self.num_actions, 1.0, 0.0)
        best_v = tf.reduce_sum(targetQ_predict_value * predict_onehot, 1)

    target = reward + (1.0 - tf.cast(isOver, tf.float32)) * self.gamma * tf.stop_gradient(best_v)
    cost = tf.losses.huber_loss(target, pred_action_value,
                                reduction=tf.losses.Reduction.MEAN)
    summary.add_param_summary(('conv.*/W', ['histogram', 'rms']),
                              ('fc.*/W', ['histogram', 'rms']))  # monitor all W
    summary.add_moving_summary(cost)
    return cost

def optimizer(self):
    lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False)
    opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)
    return optimizer.apply_grad_processors(
        opt, [gradproc.GlobalNormClip(10), gradproc.SummaryGradient()])

def update_target_param():
    """periodically triggered by trainer"""
    vars = tf.global_variables()
    ops = []
    G = tf.get_default_graph()
    for v in vars:
        target_name = v.op.name
        if target_name.startswith('target'):
            new_name = target_name.replace('target/', '')
            logger.info("{} <- {}".format(target_name, new_name))
            ops.append(v.assign(G.get_tensor_by_name(new_name + ':0')))
    return tf.group(*ops, name='update_target_network')

"""**REPLAY BUFFER TO STORE EXPERIENCES**"""

self.max_size = int(max_size)
self.state_shape = state_shape
self.history_len = int(history_len)

self.state = np.zeros((self.max_size,) + state_shape, dtype='uint8')
self.action = np.zeros((self.max_size,), dtype='int32')
self.reward = np.zeros((self.max_size,), dtype='float32')
self.isOver = np.zeros((self.max_size,), dtype='bool')

self._curr_size = 0
self._curr_pos = 0
self._hist = deque(maxlen=history_len - 1)

def append(self, exp):
    """Append the replay memory with experience sample
    Args:
        exp (Experience): experience contains (state, reward, action, isOver)
    """
    # increase current memory size if it is not full yet
    if self._curr_size < self.max_size:
        self._assign(self._curr_pos, exp)
        self._curr_pos = (self._curr_pos + 1) % self.max_size
        self._curr_size += 1
    else:
        self._assign(self._curr_pos, exp)
        self._curr_pos = (self._curr_pos + 1) % self.max_size
    if exp.isOver:
        self._hist.clear()
    else:
        self._hist.append(exp)

def recent_state(self):
    """ return a list of (hist_len-1,) + STATE_SIZE """
    lst = list(self._hist)
    states = [np.zeros(self.state_shape, dtype='uint8')] * (self._hist.maxlen - len(lst))
    states.extend([k.state for k in lst])
    return states

def sample(self, idx):
    """ Sample an experience replay from memory with index idx
    :returns: a tuple of (state, reward, action, isOver)
              where state is of shape STATE_SIZE + (history_length+1,)
    """
    idx = (self._curr_pos + idx) % self._curr_size
    k = self.history_len + 1
    if idx + k <= self._curr_size:
        state = self.state[idx: idx + k]
        reward = self.reward[idx: idx + k]
        action = self.action[idx: idx + k]
        isOver = self.isOver[idx: idx + k]
    else:
        end = idx + k - self._curr_size
        state = self._slice(self.state, idx, end)
        reward = self._slice(self.reward, idx, end)
        action = self._slice(self.action, idx, end)
        isOver = self._slice(self.isOver, idx, end)
    ret = self._pad_sample(state, reward, action, isOver)
    return ret

# the next_state is a different episode if current_state.isOver==True
def _pad_sample(self, state, reward, action, isOver):
    for k in range(self.history_len - 2, -1, -1):
        if isOver[k]:
            state = copy.deepcopy(state)
            state[:k + 1].fill(0)
            break
    # transpose state
    if state.ndim == 4:  # 3d state
        state = state.transpose(1, 2, 3, 0)
    else:  # 2d states
        state = state.transpose(1, 2, 0)
    return state, reward[-2], action[-2], isOver[-2]

def _slice(self, arr, start, end):
    s1 = arr[start:]
    s2 = arr[:end]
    return np.concatenate((s1, s2), axis=0)

def __len__(self):
    return self._curr_size

def _assign(self, pos, exp):
    self.state[pos] = exp.state
    self.reward[pos] = exp.reward
    self.action[pos] = exp.action
    self.isOver[pos] = exp.isOver
def _populate_exp(self):
    """ populate a transition by epsilon-greedy"""
    old_s = self._current_ob

    # initialize q_values to zeros
    q_values = [0, ] * self.num_actions

    if self.rng.rand() <= self.exploration or (len(self.mem) <= self.history_len):
        act = self.rng.choice(range(self.num_actions))
    else:
        # build a history state
        history = self.mem.recent_state()
        history.append(old_s)
        if np.ndim(history) == 4:  # 3d states
            history = np.stack(history, axis=3)
            # assume batched network - this is the bottleneck
            q_values = self.predictor(history[None, :, :, :, :])[0][0]
        else:
            history = np.stack(history, axis=2)
            # assume batched network - this is the bottleneck
            q_values = self.predictor(history[None, :, :, :])[0][0]

        act = np.argmax(q_values)

    self._current_ob, reward, isOver, info = self.player.step(act, q_values)

    if isOver:
        # if info['gameOver']:  # only record score when a whole game is over (not when an episode is over)
        #     self._player_scores.feed(info['score'])
        self._player_scores.feed(info['score'])
        self._player_distError.feed(info['distError'])
        self.player.reset()

    self.mem.append(Experience(old_s, act, reward, isOver))

def _debug_sample(self, sample):
    import cv2

    def view_state(comb_state):
        state = comb_state[:, :, :-1]
        next_state = comb_state[:, :, 1:]
        r = np.concatenate([state[:, :, k] for k in range(self.history_len)], axis=1)
        r2 = np.concatenate([next_state[:, :, k] for k in range(self.history_len)], axis=1)
        r = np.concatenate([r, r2], axis=0)
        cv2.imshow("state", r)
        cv2.waitKey()

    print("Act: ", sample[2], " reward:", sample[1], " isOver: ", sample[3])
    if sample[1] or sample[3]:
        view_state(sample[0])
def _process_batch(self, batch_exp):
    state = np.asarray([e[0] for e in batch_exp], dtype='uint8')
    reward = np.asarray([e[1] for e in batch_exp], dtype='float32')
    action = np.asarray([e[2] for e in batch_exp], dtype='int8')
    isOver = np.asarray([e[3] for e in batch_exp], dtype='bool')
    return [state, action, reward, isOver]

def _setup_graph(self):
    self.predictor = self.trainer.get_predictor(*self.predictor_io_names)

def _before_train(self):
    self._init_memory()
    self._simulator_th = self.get_simulator_thread()
    self._simulator_th.start()

BATCH_SIZE = 48
IMAGE_SIZE = (45, 45, 45)
FRAME_HISTORY = 4
UPDATE_FREQ = 4
GAMMA = 0.9
MEMORY_SIZE = 1e5
INIT_MEMORY_SIZE = MEMORY_SIZE // 20 #5e4
STEPS_PER_EPOCH = 196 // UPDATE_FREQ * 10
EVAL_EPISODE = 120

def _get_DQN_prediction(self, image):
    """ image: [0,255]

    :returns predicted Q values"""
    # normalize image values to [0, 1]
    image = image / 255.0

    with argscope(Conv3D, nl=PReLU.symbolic_function, use_bias=True):
        # core layers of the network
        conv = (LinearWrap(image)
              .Conv3D('conv0', out_channel=32,
                      kernel_shape=[5,5,5], stride=[1,1,1])
              .MaxPooling3D('pool0',2)
              .Conv3D('conv1', out_channel=32,
                      kernel_shape=[5,5,5], stride=[1,1,1])
              .MaxPooling3D('pool1',2)
              .Conv3D('conv2', out_channel=64,
                      kernel_shape=[4,4,4], stride=[1,1,1])
              .MaxPooling3D('pool2',2)
              .Conv3D('conv3', out_channel=64,
                      kernel_shape=[3,3,3], stride=[1,1,1])
              # .MaxPooling3D('pool3',2)
              )

    if 'Dueling' not in self.method:
        lq = (conv
              .FullyConnected('fc0', 512).tf.nn.leaky_relu(alpha=0.01)
              .FullyConnected('fc1', 256).tf.nn.leaky_relu(alpha=0.01)
              .FullyConnected('fc2', 128).tf.nn.leaky_relu(alpha=0.01)())
        Q = FullyConnected('fct', lq, self.num_actions, nl=tf.identity)
    else:
        # Dueling DQN or Double Dueling
        # state value function
        lv = (conv
              .FullyConnected('fc0V', 512).tf.nn.leaky_relu(alpha=0.01)
              .FullyConnected('fc1V', 256).tf.nn.leaky_relu(alpha=0.01)
              .FullyConnected('fc2V', 128).tf.nn.leaky_relu(alpha=0.01)())
        V = FullyConnected('fctV', lv, 1, nl=tf.identity)
        # advantage value function
        la = (conv
              .FullyConnected('fc0A', 512).tf.nn.leaky_relu(alpha=0.01)
              .FullyConnected('fc1A', 256).tf.nn.leaky_relu(alpha=0.01)
              .FullyConnected('fc2A', 128).tf.nn.leaky_relu(alpha=0.01)())
        As = FullyConnected('fctA', la, self.num_actions, nl=tf.identity)

        Q = tf.add(As, V - tf.reduce_mean(As, 1, keepdims=True))

    return tf.identity(Q, name='Qvalue')

def get_config(files_list):
    """This is only used during training."""
    expreplay = ExpReplay(
        predictor_io_names=(['state'], ['Qvalue']),
        player=get_player(task='train', files_list=files_list),
        state_shape=IMAGE_SIZE,
        batch_size=BATCH_SIZE,
        memory_size=MEMORY_SIZE,
        init_memory_size=INIT_MEMORY_SIZE,
        init_exploration=1.0,
        update_frequency=UPDATE_FREQ,
        history_len=FRAME_HISTORY
    )

    return TrainConfig(
        # dataflow=expreplay,
        data=QueueInput(expreplay),
        model=Model(),
        callbacks=[
            ModelSaver(),
            PeriodicTrigger(
                RunOp(DQNModel.update_target_param, verbose=True),
                # update target network every 10k steps
                every_k_steps=10000 // UPDATE_FREQ),
            expreplay,
            ScheduledHyperParamSetter('learning_rate',
                                      [(60, 4e-4), (100, 2e-4)]),
            ScheduledHyperParamSetter(
                ObjAttrParam(expreplay, 'exploration'),
                # 1->0.1 in the first million steps
                [(0, 1), (10, 0.1), (320, 0.01)],
                interp='linear'),
            PeriodicTrigger(
                Evaluator(nr_eval=EVAL_EPISODE, input_names=['state'],
                          output_names=['Qvalue'], files_list=files_list,
                          get_player_fn=get_player),
                every_k_epochs=EPOCHS_PER_EVAL),
            HumanHyperParamSetter('learning_rate'),
        ],
        steps_per_epoch=STEPS_PER_EPOCH,
        max_epoch=120,
    )

"""**TRAINING**"""

import torch
import numpy as np
from expreplay import ReplayMemory
from DQNModel import DQN
from evaluator import Evaluator
from tqdm import tqdm


class Trainer(object):
    def __init__(self,
                 env,
                 eval_env=None,
                 image_size=(45, 45, 45),
                 update_frequency=4,
                 replay_buffer_size=1e6,
                 init_memory_size=5e4,
                 max_episodes=100,
                 steps_per_episode=50,
                 eps=1,
                 min_eps=0.1,
                 delta=0.001,
                 batch_size=4,
                 gamma=0.9,
                 number_actions=6,
                 frame_history=4,
                 model_name="CommNet",
                 logger=None,
                 train_freq=1,
                 team_reward=False,
                 attention=False,
                 lr=1e-3,
                 scheduler_gamma=0.5,
                 scheduler_step_size=100
                ):
        self.env = env
        self.eval_env = eval_env
        self.agents = env.agents
        self.image_size = image_size
        self.update_frequency = update_frequency
        self.replay_buffer_size = replay_buffer_size
        self.init_memory_size = init_memory_size
        self.max_episodes = max_episodes
        self.steps_per_episode = steps_per_episode
        self.eps = eps
        self.min_eps = min_eps
        self.delta = delta
        self.batch_size = batch_size
        self.gamma = gamma
        self.number_actions = number_actions
        self.frame_history = frame_history
        self.epoch_length = self.env.files.num_files
        self.best_val_distance = float('inf')
        self.buffer = ReplayMemory(
            self.replay_buffer_size,
            self.image_size,
            self.frame_history,
            self.agents)
        self.dqn = DQN(
            self.agents,
            self.frame_history,
            logger=logger,
            type=model_name,
            collective_rewards=team_reward,
            attention=attention,
            lr=lr,
            scheduler_gamma=scheduler_gamma,
            scheduler_step_size=scheduler_step_size)
        self.dqn.q_network.train(True)
        self.evaluator = Evaluator(eval_env,
                                   self.dqn.q_network,
                                   logger,
                                   self.agents,
                                   steps_per_episode)
        self.logger = logger
        self.train_freq = train_freq

    def train(self):
        self.logger.log(self.dqn.q_network)
        self.init_memory()
        episode = 1
        acc_steps = 0
        epoch_distances = []
        while episode <= self.max_episodes:
            # Reset the environment for the start of the episode.
            obs = self.env.reset()
            self.buffer._hist.clear()
            terminal = [False for _ in range(self.agents)]
            losses = []
            score = [0] * self.agents
            for step_num in range(self.steps_per_episode):
                acc_steps += 1
                # Step the agent once, and get the transition tuple
                index = self.buffer.append_obs(obs)
                acts, q_values = self.get_next_actions(
                    self.buffer.recent_state())
                next_obs, reward, terminal, info = self.env.step(
                    np.copy(acts), q_values, terminal)
                self.buffer.append_effect((index, obs, acts, reward, terminal))
                score = [sum(x) for x in zip(score, reward)]
                obs = next_obs
                if acc_steps % self.train_freq == 0:
                    mini_batch = self.buffer.sample(self.batch_size)
                    loss = self.dqn.train_q_network(mini_batch, self.gamma)
                    losses.append(loss)
                if all(t for t in terminal):
                    break
            epoch_distances.append([info['distError_' + str(i)]
                                    for i in range(self.agents)])
            self.append_episode_board(info, score, "train", episode)
            if (episode * self.epoch_length) % self.update_frequency == 0:
                self.dqn.copy_to_target_network()
            self.eps = max(self.min_eps, self.eps - self.delta)
            # Every epoch
            if episode % self.epoch_length == 0:
                self.append_epoch_board(epoch_distances, self.eps, losses,
                                        "train", episode)
                self.validation_epoch(episode)
                self.dqn.save_model(name="latest_dqn.pt", forced=True)
                self.dqn.scheduler.step()
                epoch_distances = []
            episode += 1

    def init_memory(self):
        self.logger.log("Initialising memory buffer...")
        pbar = tqdm(desc="Memory buffer", total=self.init_memory_size)
        while len(self.buffer) < self.init_memory_size:
            # Reset the environment for the start of the episode.
            obs = self.env.reset()
            self.buffer._hist.clear()
            terminal = [False for _ in range(self.agents)]
            steps = 0
            for _ in range(self.steps_per_episode):
                steps += 1
                index = self.buffer.append_obs(obs)
                acts, q_values = self.get_next_actions(obs)
                next_obs, reward, terminal, info = self.env.step(
                    acts, q_values, terminal)
                self.buffer.append_effect((index, obs, acts, reward, terminal))
                obs = next_obs
                if all(t for t in terminal):
                    break
            pbar.update(steps)
        pbar.close()
        self.logger.log("Memory buffer filled")

    def append_episode_board(self, info, score, name="train", episode=0):
        dists = {str(i):
                 info['distError_' + str(i)] for i in range(self.agents)}
        self.logger.write_to_board(f"{name}/dist", dists, episode)
        scores = {str(i): score[i] for i in range(self.agents)}
        self.logger.write_to_board(f"{name}/score", scores, episode)

    def append_epoch_board(self, epoch_dists, eps=0, losses=[],
                           name="train", episode=0):
        epoch_dists = np.array(epoch_dists)
        if name == "train":
            lr = self.dqn.scheduler.state_dict()["_last_lr"]
            if isinstance(lr, list):
                lr = lr[0]
            self.logger.write_to_board(name, {"eps": eps, "lr": lr}, episode)
            if len(losses) > 0:
                loss_dict = {"loss": sum(losses) / len(losses)}
                self.logger.write_to_board(name, loss_dict, episode)
        for i in range(self.agents):
            mean_dist = sum(epoch_dists[:, i]) / len(epoch_dists[:, i])
            mean_dist_dict = {str(i): mean_dist}
            self.logger.write_to_board(
                f"{name}/mean_dist", mean_dist_dict, episode)
            min_dist_dict = {str(i): min(epoch_dists[:, i])}
            self.logger.write_to_board(
                f"{name}/min_dist", min_dist_dict, episode)
            max_dist_dict = {str(i): max(epoch_dists[:, i])}
            self.logger.write_to_board(
                f"{name}/max_dist", max_dist_dict, episode)
        return np.array(list(mean_dist_dict.values())).mean()

    def get_next_actions(self, obs_stack):
        # epsilon-greedy policy
        if np.random.random() < self.eps:
            q_values = np.zeros((self.agents, self.number_actions))
            actions = np.random.randint(self.number_actions, size=self.agents)
        else:
            actions, q_values = self.get_greedy_actions(
                obs_stack, doubleLearning=True)
        return actions, q_values

    def get_greedy_actions(self, obs_stack, doubleLearning=True):
        inputs = torch.tensor(obs_stack).unsqueeze(0)
        if doubleLearning:
            q_vals = self.dqn.q_network.forward(inputs).detach().squeeze(0)
        else:
            q_vals = self.dqn.target_network.forward(
                inputs).detach().squeeze(0)
        idx = torch.max(q_vals, -1)[1]
        greedy_steps = np.array(idx, dtype=np.int32).flatten()
        return greedy_steps, q_vals.data.numpy()

import pandas as pd
import matplotlib.pyplot as plt

def validation_epoch(self, episode):
    if self.eval_env is None:
        return
    self.dqn.q_network.train(False)
    epoch_distances = []
    for k in range(self.eval_env.files.num_files):
        self.logger.log(f"eval episode {k}")
        (score, start_dists, q_values,
            info) = self.evaluator.play_one_episode()
        epoch_distances.append([info['distError_' + str(i)]
                                for i in range(self.agents)])

    val_dists = self.append_epoch_board(epoch_distances, name="eval",
                                        episode=episode)
    if (val_dists < self.best_val_distance):
        self.logger.log("Improved new best mean validation distances")
        self.best_val_distance = val_dists
        self.dqn.save_model(name="best_dqn.pt", forced=True)
    self.dqn.q_network.train(True)

plt.figure(figsize=(10, 5))  # Set the figure size
plt.plot(df['epoch'], df['NME'], marker='o')  # Plot with markers
plt.title('NME vs Epoch')  # Add a title
plt.xlabel('Epoch')  # X-axis label
plt.ylabel('NME')  # Y-axis label
plt.grid()  # Add a grid for better readability
plt.show()

def plot_img_preds(x_test, y_test, y_pred, num_images=3):
    plt.figure(figsize=(15, 5))
    for i in range(num_images):
        plt.subplot(1, num_images, i + 1)
        # Get the image and corresponding actual/predicted landmarks
        image = x_test[i]
        actual_landmarks = y_test[i]
        predicted_landmarks = y_pred[i]
        # Convert image if necessary (e.g., from BGR to RGB)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Assuming images are in BGR
        plt.imshow(image_rgb)
        plt.axis('off')  # Hide axes
        # Plot actual landmarks
        for (x, y) in actual_landmarks:
            plt.plot(x, y, 'go', markersize=5, label='Actual')  # Green for actual
        # Plot predicted landmarks
        for (x, y) in predicted_landmarks:
            plt.plot(x, y, 'ro', markersize=5, label='Predicted')  # Red for predicted
        plt.title(f"Image {i + 1}")
        plt.legend()
    plt.tight_layout()
    plt.show()
pred = dqn_model.predict(img)  # Use your DQN model to predict landmarks
y_pred.append(pred)
# Convert y_pred to the required format if necessary
y_pred = np.array(y_pred)
# Plot the images with predicted landmarks
plot_img_preds(x_test, y_test, y_pred, num_images=3)